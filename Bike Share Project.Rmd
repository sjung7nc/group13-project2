---
title: "Bike Share Project"
author: "Soohee Jung, Kera Whitley"
output: github_document
---

# Set up
Libraries and other set up should be in this chunk
```{r}
library(tidyverse)
library(corrplot)
library(caret)
```

# Introduction
```{r}

```

# Data
```{r}
day.data <- read_csv("day.csv")
day <- day.data %>% filter(weekday == 1)

# Converting variables that should be factors into factor variables
day$season <- factor(day$season)
levels(day$season) <- c("Winter","Spring", "Summer", "Fall")

day$yr <- factor(day$yr)
levels(day$yr) <- c("2011", "2012")

day$mnth <- factor(day$mnth)
levels(day$mnth) <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")

day$holiday <- factor(day$holiday)
levels(day$holiday) <- c("Not Holiday", "Holiday")

day$workingday <- factor(day$workingday)
levels(day$workingday) <- c("Not Working", "Working")

day$weathersit <- factor(day$weathersit)
levels(day$weathersit) <- c("Fair", "Misty", "Light Snow/Rain", "Heavy Rain/Ice/Snow")

set.seed(13)
# The training set should be 70% of the data
n <- nrow(day) * 0.7
train <- sample_n(day, n, replace = FALSE)
test <- anti_join(day, train, by = "dteday")

```

# Summarizations
```{r}
# Correlation plot of the training data without the date included
#train.nodate <- train %>% select(!contains("dte"))
#train.cor <- cor(train.nodate)
#corrplot(train.cor, diag = FALSE, tl.srt = 45, na.label = " ", type = "lower")

# Total number of each casual users and registered users 
train %>% select(casual, registered) %>% colSums()

train %>% group_by(mnth) %>% summarize(avg.casual=mean(casual),avg.registered=mean(registered))
by.month <- train %>% group_by(mnth) %>% summarise(Bikers=sum(cnt))
by.month
# We can inspect the trend of all users across months using this plot.
# There may be a seasonal effect present.
ggplot(by.month, aes(x=mnth, y=Bikers)) + geom_col(fill="coral1")+
  scale_x_discrete(name="Month")

train %>% group_by(season) %>% summarize(avg.casual=mean(casual),avg.registered=mean(registered))
by.season <- train %>% group_by(season) %>% summarise(Bikers=sum(cnt))
by.season
# We can inspect the trend of all users across season using this plot.
# There may be weather or temperature effect present.
ggplot(by.season, aes(x=season, y=Bikers))+geom_col(fill="darkolivegreen4")+
  scale_x_discrete(name="Season")

train %>% group_by(weathersit) %>% summarize(avg.casual=mean(casual),avg.registered=mean(registered))
by.weather <- train %>% group_by(weathersit) %>% summarise(Bikers=sum(cnt))
by.weather
# We can inspect the trend of all users across weather condition using this plot.
ggplot(by.weather, aes(x=weathersit, y=Bikers))+geom_col(fill="cornflowerblue", width = 0.8)+
  scale_x_discrete(name="Weather")

# We can inspect the trend of all users across temperature using this plot.
ggplot(train, aes(x=temp, y=cnt)) + geom_point() + geom_smooth()+
  scale_x_continuous(name="Temperature")+scale_y_discrete(name="Bikers")

train %>% group_by(holiday) %>% summarize(avg.casual=mean(casual),avg.registered=mean(registered))
by.holi <- train %>% group_by(holiday) %>% summarise(Bikers=sum(cnt))
by.holi
# We can inspect the trend of all users across whether holiday or not using this plot.
ggplot(by.holi, aes(x=holiday, y=Bikers)) + geom_col(fill="darkgoldenrod1", width = 0.7)+
  scale_x_discrete(name="Holiday")
# We can inspect the trend of casual users across whether holiday or not using this plot.
ggplot(train, aes(x=holiday, y=casual))+geom_boxplot(fill="darkmagenta")+
  scale_x_discrete(name="Holiday")+scale_y_continuous(name="Casual Users")
# We can inspect the trend of registered users across whether holiday or not using this plot.
ggplot(train, aes(x=holiday, y=registered))+geom_boxplot(fill="darkorchid")+
  scale_x_discrete(name="Holiday")+scale_y_continuous(name="Registered Users")


table(train$season, train$weathersit)
table(train$workingday, train$weathersit)
```

# Modeling  
## Linear Regression Model  
*Linear regression* tries to find a linear equation which describe the relationship between a response variable and a explanation variable. The best model fit is made by minimizing the sum of squared residuals. Simple linear regression model can be extended in many ways and we call them *Multiple Linear Regression*.  
```{r}
set.seed(13)
# multiple linear regression model 1
## I am playing around! This is not the answer yet!!!!!!!!!!
lmFit <- train(cnt ~ temp + I(temp^2), data=train, 
               method="lm",
               trControl=trainControl(method="cv",number=10))
lmFit
lmPred <- predict(lmFit, newdata=test)

lmFit2 <- train(cnt ~ ., data=train, 
               method="lm",
               trControl=trainControl(method="cv",number=10))
lmFit2
lmPred2 <- predict(lmFit2, newdata=test)


# multiple linear regression model 2

```

## Random Forest Model  
*Random forest model* is one of 3 major methods of *Ensemble tree model*. Create a tree from a random subset of predictors for a bootstrap sample and then train the tree. Repeat this for many times, say 100 or 1000 repeats. The final prediction is average of these predictions.  
```{r}
# Get random forest model fit
## I am playing around! This is not the answer yet!!!!!!!!!!
rfFit <- train(cnt ~ temp + season + temp:season, data=train,
               method="rf", 
               trControl=trainControl(method="cv",number=10))
rfFit
rfPred <- predict(rfFit, newdata=test)

rfFit2 <- train(cnt ~ ., data=train,
               method="rf", 
               trControl=trainControl(method="cv",number=10))
rfFit2
rfPred2 <- predict(rfFit2, newdata=test)

```

## Boosted Tree Model  
```{r}



```

# Comparison  
```{r}

set.seed(13)
# multiple linear regression model 1
multiRMSE <- postResample(lmPred, test$cnt)
multiRMSE
multiRMSE2 <- postResample(lmPred2, test$cnt)
multiRMSE2
# multiple linear regression model 2

# random forest model
rfRMSE <- postResample(rfPred, test$cnt)
rfRMSE
rfRMSE2 <- postResample(rfPred2, test$cnt)
rfRMSE2

# boosted tree model


# compare
#c(   ,   ,   ,   )
```

# Automation
```{r}

```

